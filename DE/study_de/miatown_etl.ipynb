{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f1776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import schedule\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jar\", \"C:\\\\Users\\\\thehu\\\\OneDrive\\\\Mia_town\\\\IT\\\\Data\\\\DE\\\\study_de\\\\postgresql-42.7.5.jar\") \\\n",
    "                    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "#DATABASE\n",
    "host = 'localhost'\n",
    "dbname = 'miatown'\n",
    "user = 'postgres'\n",
    "password = input('hay nhap pass')\n",
    "port = '5432'\n",
    "driver = \"org.postgresql.Driver\"\n",
    "\n",
    "conn = psycopg2.connect(f'host= {host} \\\n",
    "                        dbname= {dbname} \\\n",
    "                        user= {user} \\\n",
    "                        password= {password} \\\n",
    "                        ')\n",
    "\n",
    "#set commit automaticaly\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "\n",
    "#PATH\n",
    "path = 'C:\\\\Users\\\\thehu\\\\OneDrive\\\\Mia_town\\\\IT\\\\Data\\\\MIATOWN\\\\raw\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38693d9",
   "metadata": {},
   "source": [
    "### SOURCE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6683cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CREATE TABLE\n",
    "# conn = psycopg2.connect(f'host= {host} \\\n",
    "#                         dbname= {dbname} \\\n",
    "#                         user= {user} \\\n",
    "#                         password= {password} \\\n",
    "#                         ')\n",
    "\n",
    "# #set commit automaticaly\n",
    "# conn.set_session(autocommit=True)\n",
    "# cur = conn.cursor()\n",
    "def create_database():\n",
    "    try:\n",
    "        cur.execute(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS transaction(\n",
    "                    area VARCHAR,\n",
    "                    sub_area VARCHAR,\n",
    "                    table_id VARCHAR,\n",
    "                    table_name VARCHAR,\n",
    "                    trans_id VARCHAR,\n",
    "                    amount_origin INT,\n",
    "                    voucher_amount_paid INT,\n",
    "                    total_amount INT,\n",
    "                    trans_date DATE,\n",
    "                    voucher_name VARCHAR,\n",
    "                    customer_name VARCHAR,\n",
    "                    customer_phone VARCHAR);\n",
    "                    \n",
    "                    CREATE TABLE IF NOT EXISTS product(\n",
    "                    trans_id VARCHAR,\n",
    "                    trans_date DATE,\n",
    "                    item_name VARCHAR,\n",
    "                    category VARCHAR,\n",
    "                    quantity FLOAT,\n",
    "                    amount INT,\n",
    "                    amount_discount_on_price INT);\n",
    "\n",
    "                    \"\"\")\n",
    "    except psycopg2.Error as e:\n",
    "        print('error in creating table, table is existed')\n",
    "    return print(\"Database work finished\")\n",
    "\n",
    "\n",
    "def data_extract(path):\n",
    "    transaction_combine = None \n",
    "    print('----------  combine file  ----------')\n",
    "    print('----------  create transaction data  ----------')\n",
    "    json_file = [f'{path}{file}' for file in os.listdir(path) if file.endswith('.json')]        #print a list of exactly json file from folder\n",
    "    for file in json_file:\n",
    "        print(f'----------  Selecting from {file}  ----------')\n",
    "        raw = spark.read.json(file)\n",
    "        transaction = (\n",
    "                raw.withColumn('customer_name', col('extra_data.customer_name')) \\\n",
    "                    .withColumn('customer_phone', col('extra_data.customer_phone')) \\\n",
    "                    .withColumn('trans_id', col('tran_id')) \\\n",
    "                    # .withColumn('trans_id', col('sale_detail').getItem(0).getItem('tran_id')) \\\n",
    "                    .withColumn(\"trans_date\", (from_unixtime(col(\"created_at\") / 1000)).cast('timestamp') - expr(\"INTERVAL 7 HOURS\")) #set timezone manually\n",
    "                    .withColumn(\"area\", \n",
    "                                when((col(\"table_name\").contains(\"CAFE KIDS\")) | (col(\"table_name\").contains(\"QUẦY VÉ\")), \"KID\") \\\n",
    "                                .when(col(\"table_name\").contains(\"BIDA\"), \"BIDA\") \\\n",
    "                                .when(col(\"table_name\").contains(\"PS5\"), \"GAMING\") \\\n",
    "                                .otherwise(col(\"table_name\"))\n",
    "                                )\n",
    "                    .withColumn(\"sub_area\",\n",
    "                                when(col(\"table_name\").contains(\"NHÂN VIÊN\"), \"NHÂN VIÊN\") \\\n",
    "                                .when(col(\"table_name\").contains(\"BIDA LỖ\"), \"BIDA LỖ\") \\\n",
    "                                .when(col(\"table_name\").contains(\"BIDA BĂNG\"), \"BIDA BĂNG\") \\\n",
    "                                .when(col(\"table_name\").contains(\"BIDA LIBRE\"), \"BIDA LIBRE\") \\\n",
    "                                .when(col(\"table_name\").contains(\"ROOM\"), \"ROOM\") \\\n",
    "                                .when(col(\"table_name\").contains(\"GHẾ\"), \"GHẾ\") \\\n",
    "                                .when(col(\"table_name\").contains(\"CAFE KIDS\"), \"CAFE KIDS\") \\\n",
    "                                .when(col(\"table_name\").contains(\"QUẦY VÉ\"), \"QUẦY VÉ\")\n",
    "                                .otherwise(col(\"table_name\"))\n",
    "                                )\n",
    "        )\n",
    "        transaction = transaction.select(\n",
    "                    'area',\n",
    "                    'sub_area',\n",
    "                    'table_id',\n",
    "                    'table_name', \n",
    "                    'trans_id', \n",
    "                    'amount_origin',\n",
    "                    'voucher_amount_paid', \n",
    "                    'total_amount', \n",
    "                    'trans_date', \n",
    "                    'voucher_name', \n",
    "                    'customer_name',\n",
    "                    'customer_phone' )\n",
    "        print(f'----------  Union {file}  ----------')\n",
    "        transaction_combine = transaction if transaction_combine is None else transaction_combine.union(transaction)\n",
    "    print('----------  DONE  ----------')\n",
    "    \n",
    "\n",
    "    product_combine = None \n",
    "    print('----------  combine file  ----------')\n",
    "    print('----------  CREATE PRODUCT DATA  ----------')\n",
    "    json_file = [f'{path}{file}' for file in os.listdir(path) if file.endswith('.json')]        #print a list of exactly json file from folder\n",
    "    for file in json_file:\n",
    "        print(f'----------  Selecting from {file}  ----------')\n",
    "        raw = spark.read.json(file)\n",
    "        raw = raw.withColumn(\"sale\", explode(col(\"sale_detail\")))\n",
    "        raw = raw.withColumn(\"topping\", explode_outer(col(\"sale.toppings\")))\n",
    "                \n",
    "        product = raw.select(\n",
    "                                col(\"tran_id\").alias(\"trans_id\"),\n",
    "                                ((from_unixtime(col(\"created_at\") / 1000)).cast('timestamp') - expr(\"INTERVAL 7 HOURS\")).alias(\"trans_date\"),\n",
    "                                \"sale.item_name\",\n",
    "                                when(col(\"sale.item_name\").contains(\"COMBO\"), \"COMBO\").otherwise(col(\"sale.item_class_name\")).alias(\"category\"),\n",
    "                                \"sale.quantity\",\n",
    "                                \"sale.amount\",\n",
    "                                \"sale.amount_discount_on_price\"\n",
    "                                )\n",
    "\n",
    "        topping = raw.select(\n",
    "                                col(\"tran_id\").alias(\"trans_id\"),\n",
    "                                ((from_unixtime(col(\"created_at\") / 1000)).cast('timestamp') - expr(\"INTERVAL 7 HOURS\")).alias(\"trans_date\"),\n",
    "                                \"topping.item_name\",\n",
    "                                when(col(\"sale.item_name\").contains(\"COMBO\"), \"COMBO\").otherwise(col(\"sale.item_class_name\")).alias(\"category\"),\n",
    "                                \"topping.quantity\",\n",
    "                                \"topping.amount\",\n",
    "                                \"topping.amount_discount_on_price\"\n",
    "                    )\n",
    "        windowSpec = Window.partitionBy(\"trans_id\", \"item_name\").orderBy(col(\"trans_id\"))\n",
    "        product = product.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "        topping = topping.withColumn(\"row_num\", row_number().over(windowSpec))\n",
    "\n",
    "        #UPDATE LẠI SỐ LƯỢNG COMBO\n",
    "        combo_qty = product.groupBy(\"trans_id\").agg(max(col(\"row_num\")))\n",
    "        product = product.join(combo_qty, \"trans_id\", \"left\") \n",
    "        product = product.withColumn(\n",
    "                                    \"quantity\", \n",
    "                                    when(col(\"item_name\").contains(\"COMBO\"),\n",
    "                                            when(col(\"max(row_num)\") == 1, col(\"quantity\")).otherwise(                                                                            \n",
    "                                            when((col(\"max(row_num)\") % 2 != 0) & (col(\"max(row_num)\") > 1), col(\"max(row_num)\") / 3).otherwise(col(\"max(row_num)\") / 2)\n",
    "                                            )\n",
    "                                        ).otherwise(col(\"quantity\"))\n",
    "            ) \\\n",
    "                        .withColumn(\n",
    "                            \"amount\",\n",
    "                            when(col(\"item_name\").contains(\"COMBO\"), col(\"quantity\")*col(\"amount\")).otherwise(col(\"amount\"))\n",
    "                        )\n",
    "        \n",
    "        #Filtering\n",
    "        product = product.filter((col(\"item_name\") != \"COMBO SÁNG BIDA\") | (col(\"row_num\") == 1))\n",
    "        product = product.drop(\"max(row_num)\")\n",
    "        topping = topping.filter(col(\"topping.item_name\").isNotNull())\n",
    "        final_product = product.union(topping)\n",
    "        final_product = final_product.drop(\"row_num\")\n",
    "\n",
    "        print(f'----------  Union {file}  ----------')\n",
    "        product_combine = final_product if product_combine is None else product_combine.union(final_product)   \n",
    "\n",
    "    print('----------  DONE  ----------')\n",
    "\n",
    "    return transaction_combine, product_combine\n",
    "\n",
    "\n",
    "def load_data_to_database(transaction_combine, product_combine): ## ĐỂ Ý LẠI LÚC 2 BẢNG CHÊNH LỆCH DATA KHI IMPORT VÀO, TỐI ƯU LẠI \n",
    "    print('Connect to database')\n",
    "    url = f\"jdbc:postgresql://{host}:{port}/{dbname}\"\n",
    "    properties = {\n",
    "        \"user\": f\"{user}\",\n",
    "        \"password\": f\"{password}\",\n",
    "        \"driver\": f\"{driver}\"\n",
    "    }\n",
    "    print('Completed')\n",
    "\n",
    "    print(\"Checking database\")\n",
    "    transaction_query = \"(SELECT COUNT(*) AS row_count FROM transaction) as temp\"\n",
    "    count_old_data_trans = spark.read.jdbc(url, transaction_query, properties=properties)\n",
    "    transaction_row_count = count_old_data_trans.collect()[0][\"row_count\"]\n",
    "    old_data_trans = spark.read.jdbc(url ,\"(SELECT trans_id FROM transaction)\", properties = properties)\n",
    "\n",
    "    if transaction_row_count == 0:\n",
    "        print(\"Table 'transaction' is empty. Appending new data...\")\n",
    "        # Append data\n",
    "        transaction_combine.write.jdbc(url, \"transaction\", mode = 'append', properties = properties)\n",
    "        print(\"Data successfully appended to 'TRANSACTION' table.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Table 'transaction' already contains {transaction_row_count} rows. Checking for new data.\")\n",
    "        transaction_combine.createOrReplaceTempView(\"new_data_trans\")\n",
    "        old_data_trans.createOrReplaceTempView(\"old_data_trans\")\n",
    "        trans_checking = spark.sql( \"\"\"\n",
    "                SELECT trans_id from new_data_trans\n",
    "                EXCEPT\n",
    "                SELECT trans_id from old_data_trans\n",
    "    \"\"\")\n",
    "        new_trans_data = spark.sql(\"\"\"\n",
    "                                   with id AS(\n",
    "                                    SELECT trans_id from new_data_trans\n",
    "                                    EXCEPT\n",
    "                                    SELECT trans_id from old_data_trans\n",
    "                                   )\n",
    "                                   SELECT ndt.* \n",
    "                                   FROM id i\n",
    "                                   INNER JOIN new_data_trans ndt\n",
    "                                   ON i.trans_id = ndt.trans_id\n",
    "    \"\"\")\n",
    "        if trans_checking.count() == 0:\n",
    "            print(\"No new data, finished process\")\n",
    "        else:\n",
    "            print(f\"There's {trans_checking.count()} new data in transaction, appending....\" )\n",
    "            new_trans_data.write.jdbc(url, \"transaction\", mode = 'append', properties = properties )\n",
    "            print('Completed')\n",
    "\n",
    "\n",
    "    product_query = \"(SELECT COUNT(*) AS row_count FROM product) as temp_\"\n",
    "    count_old_data_prod = spark.read.jdbc(url, product_query, properties=properties)\n",
    "    product_row_count = count_old_data_prod.collect()[0][\"row_count\"]\n",
    "    old_data_prod = spark.read.jdbc(url ,\"(SELECT trans_id FROM product)\", properties = properties)\n",
    "\n",
    "    if product_row_count == 0:\n",
    "        print(\"Table 'product' is empty. Appending new data...\")\n",
    "        # Append data\n",
    "        product_combine.write.jdbc(url, \"product\", mode = 'append', properties = properties)\n",
    "        print(\"Data successfully appended to 'PRODUCT' table.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Table 'PRODUCT' already contains {product_row_count} rows. Checking for new data.\")\n",
    "        product_combine.createOrReplaceTempView(\"new_data_prod\")\n",
    "        old_data_prod.createOrReplaceTempView(\"old_data_prod\")\n",
    "        prod_checking = spark.sql( \"\"\"\n",
    "                SELECT trans_id from new_data_prod\n",
    "                EXCEPT\n",
    "                SELECT trans_id from old_data_prod\n",
    "    \"\"\")\n",
    "        new_prod_data = spark.sql(\"\"\"\n",
    "                                   with id AS(\n",
    "                                    SELECT trans_id from new_data_prod\n",
    "                                    EXCEPT\n",
    "                                    SELECT trans_id from old_data_prod\n",
    "                                   )\n",
    "                                   SELECT ndp.* \n",
    "                                   FROM id i\n",
    "                                   INNER JOIN new_data_prod ndp\n",
    "                                   ON i.trans_id = ndp.trans_id\n",
    "    \"\"\")\n",
    "        if prod_checking.count() == 0:\n",
    "            print(\"No new data, finished process\")\n",
    "        else:\n",
    "            print(f\"There's {prod_checking.count()} new data in product, appending....\" )\n",
    "            new_prod_data.write.jdbc(url, \"product\", mode = 'append', properties = properties )\n",
    "            print('Completed')\n",
    "    print(\"----------EXTRACT COMPLETE----------\")\n",
    "    # return new_trans_data, new_prod_data\n",
    "\n",
    "\n",
    "\n",
    "def main_task(path):\n",
    "    create_database()\n",
    "    transaction_combine, product_combine = data_extract(path)\n",
    "    result = load_data_to_database(transaction_combine, product_combine)\n",
    "    print(\"Finished\")\n",
    "    \n",
    "\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf5eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\thehu\\\\OneDrive\\\\Mia_town\\\\IT\\\\Data\\\\MIATOWN\\\\raw\\\\'\n",
    "main_task(path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
