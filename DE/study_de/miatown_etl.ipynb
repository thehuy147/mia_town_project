{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f1776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jar\", \"C:\\\\Users\\\\thehu\\\\OneDrive\\\\Mia_town\\\\IT\\\\Data\\\\DE\\\\study_de\\\\postgresql-42.7.5.jar\") \\\n",
    "                    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "#DATABASE\n",
    "host = 'localhost'\n",
    "dbname = 'miatown'\n",
    "user = 'postgres'\n",
    "password = input('hay nhap pass')\n",
    "port = '5432'\n",
    "driver = \"org.postgresql.Driver\"\n",
    "\n",
    "conn = psycopg2.connect(f'host= {host} \\\n",
    "                        dbname= {dbname} \\\n",
    "                        user= {user} \\\n",
    "                        password= {password} \\\n",
    "                        ')\n",
    "\n",
    "#set commit automaticaly\n",
    "conn.set_session(autocommit=True)\n",
    "cur = conn.cursor()\n",
    "\n",
    "\n",
    "#PATH\n",
    "path = 'C:\\\\Users\\\\thehu\\\\OneDrive\\\\Mia_town\\\\IT\\\\Data\\\\MIATOWN\\\\raw\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38693d9",
   "metadata": {},
   "source": [
    "### SOURCE CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6683cecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CREATE TABLE\n",
    "# conn = psycopg2.connect(f'host= {host} \\\n",
    "#                         dbname= {dbname} \\\n",
    "#                         user= {user} \\\n",
    "#                         password= {password} \\\n",
    "#                         ')\n",
    "\n",
    "# #set commit automaticaly\n",
    "# conn.set_session(autocommit=True)\n",
    "# cur = conn.cursor()\n",
    "def create_database():\n",
    "    try:\n",
    "        cur.execute(\"\"\"\n",
    "                    CREATE TABLE IF NOT EXISTS transaction(\n",
    "                    table_id VARCHAR,\n",
    "                    table_name VARCHAR,\n",
    "                    trans_id VARCHAR,\n",
    "                    amount_origin INT,\n",
    "                    voucher_amount_paid INT,\n",
    "                    total_amount INT,\n",
    "                    trans_date DATE,\n",
    "                    voucher_name VARCHAR,\n",
    "                    customer_name VARCHAR,\n",
    "                    customer_phone VARCHAR);\n",
    "                    \n",
    "\n",
    "                    \"\"\")\n",
    "    except psycopg2.Error as e:\n",
    "        print('error in creating table, table is existed')\n",
    "    return print(\"Database work finished\")\n",
    "\n",
    "\n",
    "def df_combined(path):\n",
    "    df_combined = None \n",
    "    print('----------  combine file  ----------')\n",
    "    json_file = [f'{path}{file}' for file in os.listdir(path) if file.endswith('.json')]        #print a list of exactly json file from folder\n",
    "    for file in json_file:\n",
    "        print(f'----------  Selecting from {file}  ----------')\n",
    "        df = spark.read.json(file)\n",
    "        df = (\n",
    "                df.withColumn('customer_name', col('extra_data.customer_name')) \\\n",
    "                    .withColumn('customer_phone', col('extra_data.customer_phone')) \\\n",
    "                    .withColumn('trans_id', col('sale_detail').getItem(0).getItem('tran_id')) \\\n",
    "                    .withColumn(\"trans_date\", (from_unixtime(col(\"created_at\") / 1000)).cast('timestamp') - expr(\"INTERVAL 7 HOURS\")) #set timezone manually\n",
    "        )\n",
    "        df = df.select(\n",
    "                    'table_id',\n",
    "                    'table_name', \n",
    "                    'trans_id', \n",
    "                    'amount_origin',\n",
    "                    'voucher_amount_paid', \n",
    "                    'total_amount', \n",
    "                    'trans_date', \n",
    "                    'voucher_name', \n",
    "                    'customer_name',\n",
    "                    'customer_phone' )\n",
    "        print(f'----------  Union {file}  ----------')\n",
    "        df_combined = df if df_combined is None else df_combined.union(df)\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def load_data_to_database(new_data_trans):\n",
    "    print('Connect to database')\n",
    "    url = f\"jdbc:postgresql://{host}:{port}/{dbname}\"\n",
    "    properties = {\n",
    "        \"user\": f\"{user}\",\n",
    "        \"password\": f\"{password}\",\n",
    "        \"driver\": f\"{driver}\"\n",
    "    }\n",
    "    print('Completed')\n",
    "\n",
    "    print(\"Checking database\")\n",
    "    query = \"(SELECT COUNT(*) AS row_count FROM transaction) as temp\"\n",
    "    count_old_data_trans = spark.read.jdbc(url, query, properties=properties)\n",
    "    row_count = count_old_data_trans.collect()[0][\"row_count\"]\n",
    "    old_data_trans = spark.read.jdbc(url ,\"(SELECT trans_id FROM transaction)\", properties = properties)\n",
    "\n",
    "    if row_count == 0:\n",
    "        print(\"Table 'transaction' is empty. Appending new data...\")\n",
    "        # Append data\n",
    "        new_data_trans.write.jdbc(url, \"transaction\", mode = 'append', properties = properties)\n",
    "        print(\"Data successfully appended to 'transaction' table.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Table 'transaction' already contains {row_count} rows. Checking for new data.\")\n",
    "        new_data_trans.createOrReplaceTempView(\"new_data_trans\")\n",
    "        old_data_trans.createOrReplaceTempView(\"old_data_trans\")\n",
    "        checking = spark.sql( \"\"\"\n",
    "                SELECT trans_id from new_data_trans\n",
    "                EXCEPT\n",
    "                SELECT trans_id from old_data_trans\n",
    "    \"\"\")\n",
    "        if checking.count() == 0:\n",
    "            print(\"No new data, finished process\")\n",
    "        else:\n",
    "            print(f\"There's {checking.count()} new data in transaction, appending....\" )\n",
    "            checking.write.jdbc(url, \"transaction\", mode = 'append', properties = properties )\n",
    "            print('Completed')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def main_task(path):\n",
    "    create_database()\n",
    "    df = df_combined(path)\n",
    "    result = load_data_to_database(df)\n",
    "    print(\"Finished\")\n",
    "    \n",
    "\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aaf5eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database work finished\n",
      "----------  combine file  ----------\n",
      "----------  Selecting from C:\\Users\\thehu\\OneDrive\\Mia_town\\IT\\Data\\MIATOWN\\raw\\bida_t04_2025.json  ----------\n",
      "----------  Union C:\\Users\\thehu\\OneDrive\\Mia_town\\IT\\Data\\MIATOWN\\raw\\bida_t04_2025.json  ----------\n",
      "----------  Selecting from C:\\Users\\thehu\\OneDrive\\Mia_town\\IT\\Data\\MIATOWN\\raw\\gamingps_t03_2025.json  ----------\n",
      "----------  Union C:\\Users\\thehu\\OneDrive\\Mia_town\\IT\\Data\\MIATOWN\\raw\\gamingps_t03_2025.json  ----------\n",
      "----------  Selecting from C:\\Users\\thehu\\OneDrive\\Mia_town\\IT\\Data\\MIATOWN\\raw\\gamingps_t04_2025.json  ----------\n",
      "----------  Union C:\\Users\\thehu\\OneDrive\\Mia_town\\IT\\Data\\MIATOWN\\raw\\gamingps_t04_2025.json  ----------\n",
      "----------  Selecting from C:\\Users\\thehu\\OneDrive\\Mia_town\\IT\\Data\\MIATOWN\\raw\\kid_t04_2025.json  ----------\n",
      "----------  Union C:\\Users\\thehu\\OneDrive\\Mia_town\\IT\\Data\\MIATOWN\\raw\\kid_t04_2025.json  ----------\n",
      "Connect to database\n",
      "Completed\n",
      "Checking database\n",
      "Table 'transaction' already contains 11692 rows. Checking for new data.\n",
      "No new data, finished process\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\thehu\\\\OneDrive\\\\Mia_town\\\\IT\\\\Data\\\\MIATOWN\\\\raw\\\\'\n",
    "main_task(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe61b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = f\"jdbc:postgresql://{host}:{port}/{dbname}\"\n",
    "# properties = {\n",
    "#         \"user\": f\"{user}\",\n",
    "#         \"password\": f\"{password}\",\n",
    "#         \"driver\": f\"{driver}\"\n",
    "#     }\n",
    "\n",
    "# new_data_trans = df_combined(path)\n",
    "\n",
    "\n",
    "# query = \"(SELECT COUNT(*) AS row_count FROM transaction) as temp\"\n",
    "# count_old_data_trans = spark.read.jdbc(url, query, properties=properties)\n",
    "# row_count = count_old_data_trans.collect()[0][\"row_count\"]\n",
    "# old_data_trans = spark.read.jdbc(url ,\"(SELECT trans_id FROM transaction)\", properties = properties)\n",
    "\n",
    "# if row_count == 0:\n",
    "#     print(\"Table 'transaction' is empty. Appending new data...\")\n",
    "#        # Append data\n",
    "#     new_data_trans.write.jdbc(url, \"transaction\", mode = 'append', properties = properties)\n",
    "#     print(\"Data successfully appended to 'transaction' table.\")\n",
    "\n",
    "# else:\n",
    "#     print(f\"Table 'transaction' already contains {row_count} rows. Skipping append.\")\n",
    "#     new_data_trans.createOrReplaceTempView(\"new_data_trans\")\n",
    "#     old_data_trans.createOrReplaceTempView(\"old_data_trans\")\n",
    "#     checking = spark.sql( \"\"\"\n",
    "#             SELECT trans_id from new_data_trans\n",
    "#             EXCEPT\n",
    "#             SELECT trans_id from old_data_trans\n",
    "# \"\"\")\n",
    "#     if checking.count() == 0:\n",
    "#         print(\"No new data, finished process\")\n",
    "#     else:\n",
    "#         print(\"There's new data in transaction, appending....\" )\n",
    "#         checking.write.jdbc(url, \"transaction\", mode = 'append', properties = properties )\n",
    "#         print('Completed')\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af799cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
